{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f4de42",
   "metadata": {},
   "source": [
    "# dAiv AI_Competition[2024]_Pro Baseline for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9142930",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, rename, mkdir, listdir\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, utils\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datasets.utils.tqdm = tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049b434",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475e84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2df737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA Device Number 0~7\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "print(\"INFO: Using device -\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc3d0f",
   "metadata": {},
   "source": [
    "## Load DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b28aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "\n",
    "class ImageDataset(datasets.ImageFolder):\n",
    "    download_url = \"https://daiv-cnu.duckdns.org/contest/ai_competition[2024]_pro/dataset/archive.zip\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, root: str, force_download: bool = True,\n",
    "            train: bool = False, valid: bool = False, split_ratio: float = 0.8,\n",
    "            test: bool = False, unlabeled: bool = False,\n",
    "            transform: Optional[Callable] = None, target_transform: Optional[Callable] = None\n",
    "    ):\n",
    "        self.download(root, force=force_download)  # Download Dataset from server\n",
    "\n",
    "        if train or valid:  # Set-up directory\n",
    "            root = path.join(root, \"train\")\n",
    "        else:\n",
    "            root = path.join(root, \"test\" if test else \"unlabeled\" if unlabeled else None)\n",
    "\n",
    "        # Initialize ImageFolder\n",
    "        super().__init__(root=root, transform=transform, target_transform=target_transform)\n",
    "\n",
    "        if train:  # Split Train and Validation Set\n",
    "            self.targets, self.samples = [d[:int(len(self.targets) * split_ratio)] for d in (self.targets, self.samples)]\n",
    "        elif valid:\n",
    "            self.targets, self.samples = [d[int(len(self.targets) * split_ratio):] for d in (self.targets, self.samples)]\n",
    "\n",
    "    @classmethod\n",
    "    def download(cls, root: str, force: bool = False):\n",
    "        if force or not path.isfile(path.join(root, \"archive.zip\")):\n",
    "            # Download and Extract Dataset\n",
    "            datasets.utils.download_and_extract_archive(cls.download_url, download_root=root, extract_root=root, filename=\"archive.zip\")\n",
    "            \n",
    "            # Arrange Dataset Directory\n",
    "            for target_dir in [path.join(root, \"test\"), path.join(root, \"unlabeled\")]:\n",
    "                for file in listdir(target_dir):\n",
    "                    mkdir(path.join(target_dir, file.replace(\".jpg\", \"\")))\n",
    "                    rename(path.join(target_dir, file), path.join(target_dir, file.replace(\".jpg\", \"\"), file))\n",
    "\n",
    "            print(\"INFO: Dataset archive downloaded and extracted.\")\n",
    "        else:\n",
    "            print(\"INFO: Dataset archive found in the root directory. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1489d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Resizing and Tensor Conversion\n",
    "IMG_SIZE = (256, 256)\n",
    "IMG_NORM = dict(  # ImageNet Normalization\n",
    "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resizer = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # Resize Image\n",
    "    transforms.ToTensor(),  # Convert Image to Tensor\n",
    "    transforms.Normalize(**IMG_NORM)  # Normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55933cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = path.join(\".\", \"data\")\n",
    "\n",
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=resizer)\n",
    "valid_dataset = ImageDataset(root=DATA_ROOT, force_download=False, valid=True, transform=resizer)\n",
    "\n",
    "test_dataset = ImageDataset(root=DATA_ROOT, force_download=False, test=True, transform=resizer)\n",
    "unlabeled_dataset = ImageDataset(root=DATA_ROOT, force_download=False, unlabeled=True, transform=resizer)\n",
    "\n",
    "print(f\"INFO: Dataset loaded successfully. Number of samples - Train({len(train_dataset)}), Valid({len(valid_dataset)}), Test({len(test_dataset)}), Unlabeled({len(unlabeled_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34674f",
   "metadata": {},
   "source": [
    "## Data Augmentation if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ced1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROTATE_ANGLE = 20\n",
    "COLOR_TRANSFORM = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(ROTATE_ANGLE),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=COLOR_TRANSFORM, contrast=COLOR_TRANSFORM,\n",
    "        saturation=COLOR_TRANSFORM, hue=COLOR_TRANSFORM\n",
    "    ),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0), ratio=(0.75, 1.333)),\n",
    "    resizer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(root=DATA_ROOT, force_download=False, train=True, transform=augmenter)\n",
    "\n",
    "print(f\"INFO: Train dataset has been overridden with augmented state. Number of samples - Train({len(train_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccaf990",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf78698",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROCESSING = True  # Set False if DataLoader is causing issues\n",
    "\n",
    "from platform import system\n",
    "if MULTI_PROCESSING and system() != \"Windows\":  # Multiprocess data loading is not supported on Windows\n",
    "    import multiprocessing\n",
    "    cpu_cores = multiprocessing.cpu_count()\n",
    "    print(f\"INFO: Number of CPU cores - {cpu_cores}\")\n",
    "else:\n",
    "    cpu_cores = 0\n",
    "    print(\"INFO: Using DataLoader without multi-processing.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=cpu_cores)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3220e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Visualizer\n",
    "def imshow(image_list, mean=IMG_NORM['mean'], std=IMG_NORM['std']):\n",
    "    np_image = np.array(image_list).transpose((1, 2, 0))\n",
    "    de_norm_image = np_image * std + mean\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(de_norm_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c71c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_loader))\n",
    "grid_images = utils.make_grid(images, nrow=8, padding=10)\n",
    "imshow(grid_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26620dd",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c184a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T13:53:55.526126Z",
     "start_time": "2024-09-30T13:53:55.516359Z"
    }
   },
   "outputs": [],
   "source": [
    "class SecondMaxLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.clone()\n",
    "        max_val, max_idx = torch.max(x, dim=1, keepdim=True)\n",
    "        x.scatter_(1, max_idx, 1e-12)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca47ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, input_channel: int, output_channel: int, adaptive_pool_size: int, img_size: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.multiple_output = False\n",
    "\n",
    "        # Feature Extractor\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channel, out_channels=output_channel//4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=output_channel//4, out_channels=output_channel//4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(output_channel//4)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=output_channel//4, out_channels=output_channel//2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=output_channel//2, out_channels=output_channel//2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(output_channel//2)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=output_channel//2, out_channels=output_channel, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(output_channel)\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        # Assuming you want to connect to a fully connected layer after flattening\n",
    "        # Calculate the size of the flattened features after 3 pooling layers\n",
    "        self.fc_size = output_channel * (img_size // 2**3) * (img_size // 2**3)\n",
    "        self.fc = nn.Linear(self.fc_size, adaptive_pool_size)\n",
    "        \n",
    "        # Adaptive Layer\n",
    "        self.adaptive_layer = nn.Sequential(\n",
    "            nn.Linear(adaptive_pool_size, adaptive_pool_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.domain_classifier = nn.Linear(adaptive_pool_size, 1)  # Train domain or Test domain\n",
    "        \n",
    "        # Output Layer\n",
    "        self.classifier = nn.Linear(adaptive_pool_size, num_classes)\n",
    "        self.secondary = SecondMaxLayer()  # For multi-label classification\n",
    "\n",
    "    def toggle_multilabel(self, multi_label: bool | None = None):\n",
    "        if isinstance(multi_label, bool):\n",
    "            self.multiple_output = multi_label\n",
    "        else:\n",
    "            self.multiple_output = not self.multiple_output\n",
    "\n",
    "    def forward(self, x) -> tuple[torch.Tensor, torch.Tensor] | tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        # Feature Extracting\n",
    "        h1 = self.layer1(x)\n",
    "        h2 = self.layer2(h1)\n",
    "        h3 = self.layer3(h2)\n",
    "        h3 = h3.view(h3.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "        extracted = self.fc(h3)\n",
    "\n",
    "        # Adaptive Layer\n",
    "        adapted = self.adaptive_layer(extracted)\n",
    "\n",
    "        # Output Layer\n",
    "        out = self.classifier(adapted)\n",
    "        domain = torch.sigmoid(self.domain_classifier(adapted))  # Logits\n",
    "        if self.multiple_output:\n",
    "            return domain, out, self.secondary(adapted)\n",
    "        return domain, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de92fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_LABELS = len(train_dataset.classes)\n",
    "\n",
    "MODEL_PARAMS = dict(\n",
    "    input_channel=3, output_channel=64, adaptive_pool_size=512,\n",
    "    img_size=IMG_SIZE[0], num_classes=CLASS_LABELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17163e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bee1b6",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Interactive Loss Plot Update\n",
    "def create_plot():\n",
    "    losses = []\n",
    "\n",
    "    # Enable Interactive Mode\n",
    "    plt.ion()\n",
    "\n",
    "    # Loss Plot Setting\n",
    "    fig, ax = plt.subplots(figsize=(6, 2))\n",
    "    line, = ax.plot(losses)\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Cross Entropy Loss\")\n",
    "\n",
    "    # Display Plot\n",
    "    plot = widgets.Output()\n",
    "    display(plot)\n",
    "\n",
    "    def update_plot(new_loss):\n",
    "        losses.append(new_loss.item())\n",
    "        line.set_ydata(losses)\n",
    "        line.set_xdata(range(len(losses)))\n",
    "        ax.relim()\n",
    "        ax.autoscale_view()\n",
    "        with plot:\n",
    "            plot.clear_output(wait=True)\n",
    "            display(fig)\n",
    "\n",
    "    return update_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdee4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Epoch Count\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_length, valid_length = map(len, (train_loader, valid_loader))\n",
    "\n",
    "epochs = tqdm(range(num_epochs), desc=\"Running Epochs\")\n",
    "with (tqdm(total=train_length, desc=\"Training\") as train_progress,\n",
    "        tqdm(total=valid_length, desc=\"Validation\") as valid_progress):  # Set up Progress Bars\n",
    "    update = create_plot()  # Create Loss Plot\n",
    "\n",
    "    for epoch in epochs:\n",
    "        train_progress.reset(total=train_length)\n",
    "        valid_progress.reset(total=valid_length)\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            domain_outputs, outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            update(loss)\n",
    "            train_progress.update(1)\n",
    "            print(f\"\\rEpoch [{epoch+1:2}/{num_epochs}], Step [{i+1:3}/{train_length}], Loss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "        val_acc, val_loss = 0, 0\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                domain_outputs, outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets).item() / valid_length\n",
    "                val_acc += (torch.max(outputs, 1)[1] == targets.data).sum() / len(valid_dataset)\n",
    "                valid_progress.update(1)\n",
    "\n",
    "        print(f\"\\rEpoch [{epoch+1:2}/{num_epochs}], Step [{train_length}/{train_length}], Loss: {loss.item():.6f}, Valid Acc: {val_acc:.6%}, Valid Loss: {val_loss:.6f}\", end=\"\\n\" if (epoch+1) % 5 == 0 or (epoch+1) == num_epochs else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02f1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.isdir(path.join(\".\", \"models\")):\n",
    "    import os\n",
    "    os.mkdir(path.join(\".\", \"models\"))\n",
    "\n",
    "# Model Save\n",
    "save_path = path.join(\".\", \"models\", f\"baseline_model.pt\")\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a063dc95",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "- 추론 코드 수정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model_id = \"baseline_model\"\n",
    "\n",
    "model = ImageClassifier(**MODEL_PARAMS)\n",
    "model.load_state_dict(torch.load(path.join(\".\", \"models\", f\"{model_id}.pt\")))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e391d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict(id=[], label1=[], label2=[])\n",
    "test_length = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, ids in tqdm(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        results['id'] += [test_dataset.classes[i] for i in ids]\n",
    "        results['label1'] += [train_dataset.classes[i] for i in preds.cpu().detach().numpy().tolist()]\n",
    "        results['label2'] += [train_dataset.classes[i] for i in preds.cpu().detach().numpy().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "submission_dir = \"submissions\"\n",
    "if not path.isdir(submission_dir):\n",
    "    mkdir(submission_dir)\n",
    "\n",
    "submit_file_path = path.join(submission_dir, f\"{model_id}.csv\")\n",
    "results_df.to_csv(submit_file_path, index=False)\n",
    "print(\"File saved to\", submit_file_path)\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85668afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.09/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
